{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8158a01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Desktop\\telegram_ner_project\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, Dataset, DatasetDict, ClassLabel, Sequence\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "354f4977",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"ner_data.conll\"  # Make sure this file is in the same folder as your notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ee9f5dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_conll' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tokens, ner_tags = \u001b[43mread_conll\u001b[49m(file_path)\n",
      "\u001b[31mNameError\u001b[39m: name 'read_conll' is not defined"
     ]
    }
   ],
   "source": [
    "tokens, ner_tags = read_conll(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45fcd5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52e93213",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ner_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m model_checkpoint = \u001b[33m\"\u001b[39m\u001b[33mxlm-roberta-base\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m unique_labels = \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(tag \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m \u001b[43mner_tags\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m seq))\n\u001b[32m      5\u001b[39m label2id = {label: i \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(unique_labels)}\n\u001b[32m      6\u001b[39m id2label = {i: label \u001b[38;5;28;01mfor\u001b[39;00m label, i \u001b[38;5;129;01min\u001b[39;00m label2id.items()}\n",
      "\u001b[31mNameError\u001b[39m: name 'ner_tags' is not defined"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "\n",
    "unique_labels = sorted(set(tag for seq in ner_tags for tag in seq))\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "507f25ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "136d8c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2532/2532 [00:01<00:00, 2089.67 examples/s]\n",
      "Map: 100%|██████████| 634/634 [00:00<00:00, 2000.98 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], is_split_into_words=True, truncation=True)\n",
    "    labels = []\n",
    "\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Special token, ignored by loss function\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                # For wordpieces inside the same word, assign the same label\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Create dataset and split into train and test\n",
    "dataset = Dataset.from_dict({\"tokens\": tokens, \"ner_tags\": ner_tags})\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "# Apply tokenizer & align labels\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c3c1fbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ner_tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Assuming ner_tags is already loaded from read_conll()\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m unique_labels = \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(tag \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m \u001b[43mner_tags\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m seq))\n\u001b[32m      4\u001b[39m label2id = {label: i \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(unique_labels)}\n\u001b[32m      5\u001b[39m id2label = {i: label \u001b[38;5;28;01mfor\u001b[39;00m label, i \u001b[38;5;129;01min\u001b[39;00m label2id.items()}\n",
      "\u001b[31mNameError\u001b[39m: name 'ner_tags' is not defined"
     ]
    }
   ],
   "source": [
    "# Assuming ner_tags is already loaded from read_conll()\n",
    "unique_labels = sorted(set(tag for seq in ner_tags for tag in seq))\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18ff1f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.52.4\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bd50c90",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n\u001b[32m      3\u001b[39m model_checkpoint = \u001b[33m\"\u001b[39m\u001b[33mxlm-roberta-base\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m model = AutoModelForTokenClassification.from_pretrained(\n\u001b[32m      6\u001b[39m     model_checkpoint,\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     num_labels=\u001b[38;5;28mlen\u001b[39m(\u001b[43munique_labels\u001b[49m),\n\u001b[32m      8\u001b[39m     id2label=id2label,\n\u001b[32m      9\u001b[39m     label2id=label2id\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m data_collator = DataCollatorForTokenClassification(tokenizer)\n\u001b[32m     14\u001b[39m training_args = TrainingArguments(\n\u001b[32m     15\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./results\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m     do_train=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     save_total_limit=\u001b[32m2\u001b[39m\n\u001b[32m     28\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'unique_labels' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(unique_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    save_total_limit=2\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
